{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "* Scrape listings\n",
    "* List of Boroughs\n",
    "* Data on crime rates, salaries etc... (Kaggle, Google, Gov stats)\n",
    "* Save it all to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping all boroughs process:\n",
    "* ~~scrape boroughs into a list~~\n",
    "    * loop through boroughs list to create urls\n",
    "        * get number of pages with each url\n",
    "            * loop through listings with range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city-of-london-london-borough',\n",
       " 'city-of-westminster',\n",
       " 'kensington-and-chelsea-royal-borough',\n",
       " 'hammersmith-and-fulham-london-borough',\n",
       " 'wandsworth-london-borough',\n",
       " 'lambeth-london-borough',\n",
       " 'southwark-london-borough',\n",
       " 'tower-hamlets-london-borough',\n",
       " 'hackney-london-borough',\n",
       " 'islington-london-borough',\n",
       " 'camden-london-borough',\n",
       " 'brent-london-borough',\n",
       " 'ealing-london-borough',\n",
       " 'hounslow-london-borough',\n",
       " 'richmond-upon-thames-london-borough',\n",
       " 'kingston-upon-thames',\n",
       " 'merton-london-borough',\n",
       " 'sutton-london-borough',\n",
       " 'croydon-london-borough',\n",
       " 'bromley-london-borough',\n",
       " 'lewisham-london-borough',\n",
       " 'greenwich-royal-borough',\n",
       " 'bexley-london-borough',\n",
       " 'havering-london-borough',\n",
       " 'barking-and-dagenham-london-borough',\n",
       " 'redbridge-london-borough',\n",
       " 'newham-london-borough',\n",
       " 'waltham-forest-london-borough',\n",
       " 'haringey-london-borough',\n",
       " 'enfield-london-borough',\n",
       " 'barnet-london-borough',\n",
       " 'harrow-london-borough',\n",
       " 'hillingdon-london-borough']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_url = 'https://en.wikipedia.org/wiki/London_boroughs'\n",
    "wiki = requests.get(wiki_url)\n",
    "wiki_soup = BeautifulSoup(wiki.content, 'html.parser')\n",
    "ols = wiki_soup.select('td > ol')\n",
    "boroughs = []\n",
    "for ol in ols:\n",
    "    lis = ol.find_all('li')\n",
    "    for li in lis:\n",
    "        boroughs.append(li.find('a').getText())\n",
    "        \n",
    "# List of boroughs\n",
    "boroughs = [x.replace(' ', '-').lower() + '-london-borough' for x in boroughs]\n",
    "# Exceptions treatment\n",
    "boroughs[boroughs.index('kensington-and-chelsea-london-borough')] = 'kensington-and-chelsea-royal-borough'\n",
    "boroughs[boroughs.index('city-of-westminster-london-borough')] = 'city-of-westminster'\n",
    "boroughs[boroughs.index('kingston-upon-thames-london-borough')] = 'kingston-upon-thames'\n",
    "boroughs[boroughs.index('greenwich-london-borough')] = 'greenwich-royal-borough'\n",
    "boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "borough_list = []\n",
    "price_list = []\n",
    "bed_list = []\n",
    "bath_list = []\n",
    "chair_list = []\n",
    "link_list = []\n",
    "title_list = []\n",
    "address_list = []\n",
    "transport1_list = []\n",
    "transport2_list = []\n",
    "transport_type_1_list = []\n",
    "transport_type_2_list = []\n",
    "tag_list = []\n",
    "parking_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the links\n",
    "links = []\n",
    "pages = []\n",
    "for borough in boroughs:\n",
    "    links.append(f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn=1')\n",
    "\n",
    "# Looping through all links and scraoing the listings\n",
    "for link in links:\n",
    "    try:\n",
    "        data = requests.get(link).content\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        last_page_nr = int(soup.find_all('li', {'class': 'eaoxhri1'})[-1].find('a').getText()) + 1\n",
    "        pages.append(last_page_nr)\n",
    "    except:\n",
    "        print(link)\n",
    "        \n",
    "len(links) == len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting city-of-london-london-borough\n",
      "city-of-london-london-borough has been scraped\n",
      "Starting city-of-westminster\n",
      "city-of-westminster, page 199, listing listing_51502921 has an error 'NoneType' object has no attribute 'find_all'\n",
      "city-of-westminster has been scraped\n",
      "Starting kensington-and-chelsea-royal-borough\n",
      "kensington-and-chelsea-royal-borough, page 37, listing listing_58246684 has an error 'NoneType' object has no attribute 'find_all'\n",
      "kensington-and-chelsea-royal-borough, page 49, listing listing_58090695 has an error 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "for i, borough in enumerate(boroughs):\n",
    "    print(f'Starting {borough}')\n",
    "    for n in range(1, pages[i]):        \n",
    "        try:\n",
    "            base_url = f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn={n}'\n",
    "            data = requests.get(base_url).content\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            listings_container = soup.find('div', {'class': 'css-kdnpqc-ListingsContainer'})\n",
    "            listings = listings_container.find_all('div', {'class': 'e2uk8e18'})\n",
    "            regex = re.compile(r'search-result_listing_\\d+')\n",
    "            listing_ids = soup.find_all('div', {'data-testid': regex})\n",
    "            \n",
    "            #For loop for the listings list\n",
    "            for i, listing in enumerate(listings):\n",
    "                try:\n",
    "                    id_num = listing_ids[i].attrs['id']\n",
    "                    ids.append(id_num)\n",
    "                    #Borough\n",
    "                    borough_list.append(borough)\n",
    "                    #Price\n",
    "                    price = listing.find('p', {'class': 'eczcs4p0'}).getText()\n",
    "                    price_list.append(price)\n",
    "\n",
    "                    #Features\n",
    "                    bed = listing.find('span', {'data-testid': 'bed'})\n",
    "                    bath = listing.find('span', {'data-testid': 'bath'})\n",
    "                    chair = listing.find('span', {'data-testid': 'chair'})\n",
    "                    if bed:\n",
    "                        bed_list.append(bed.find_next('p').getText())\n",
    "                    else:\n",
    "                        bed_list.append('1')\n",
    "                    if bath:\n",
    "                        bath_list.append(bath.find_next('p').getText())\n",
    "                    else:\n",
    "                        bath_list.append('1')\n",
    "                    if chair:\n",
    "                        chair_list.append(chair.find_next('p').getText())\n",
    "                    else:\n",
    "                        chair_list.append('0')\n",
    "\n",
    "                    #Links\n",
    "                    link = listing.find('a', {'data-testid': 'listing-details-link'}).attrs['href']\n",
    "                    link_list.append(link)\n",
    "\n",
    "                    #Title\n",
    "                    title = listing.find('h2', {'class': 'e2uk8e14'}).getText()\n",
    "                    title_list.append(title)\n",
    "\n",
    "                    #Address\n",
    "                    address = listing.find('h2', {'class': 'e2uk8e14'}).find_next('p').getText()\n",
    "                    address_list.append(address)\n",
    "\n",
    "                    #Transportation\n",
    "                    transport_container = listing.find('div', {'class': 'css-braguw-TransportWrapper'}).find_all('div', {'class': 'ejjz7ko0'})\n",
    "                    transp1, transp2 = transport_container[:2]\n",
    "\n",
    "                    transport1 = transp1.attrs['content']\n",
    "                    transport2 = transp2.attrs['content']\n",
    "\n",
    "                    transport1_list.append(transport1)\n",
    "                    transport2_list.append(transport2)\n",
    "\n",
    "                    transport_type_1 = transp1.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "                    transport_type_2 = transp2.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "\n",
    "                    transport_type_1_list.append(transport_type_1)\n",
    "                    transport_type_2_list.append(transport_type_2)\n",
    "\n",
    "                    #Parking\n",
    "                    parking = listing.find('div', {'content': 'Parking'})\n",
    "                    if parking:\n",
    "                        parking_list.append(1)\n",
    "                    else:\n",
    "                        parking_list.append(0)\n",
    "\n",
    "                    #Tags\n",
    "                    tag = listings[0].find('span', {'class': 'css-10ddrfy-Tag-StyledTag'})\n",
    "                    if tag:\n",
    "                        tag_list.append(tag.getText())\n",
    "                    else:\n",
    "                        tag_list.append('None')\n",
    "                        \n",
    "                except:\n",
    "                    print(f'Problem at {id_num} on page {n} of {borough}')\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        except Exception as err:\n",
    "            print(f'{borough}, page {n}, listing {id_num} has an error {str(err)}')\n",
    "            \n",
    "\n",
    "    print(f'{borough} has been scraped')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_test(borough, n, id_num):\n",
    "    base_url = f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn={n}'\n",
    "    print(base_url)\n",
    "    data = requests.get(base_url).content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    listing = soup.find('div', {'id': id_num})\n",
    "    \n",
    "    #For loop for the listings list\n",
    "\n",
    "    try:\n",
    "        print(id_num)\n",
    "        #Borough\n",
    "        print(borough)\n",
    "        #Price\n",
    "        price = listing.find('p', {'class': 'eczcs4p0'}).getText()\n",
    "        print(price)\n",
    "\n",
    "        #Features\n",
    "        bed = listing.find('span', {'data-testid': 'bed'})\n",
    "        bath = listing.find('span', {'data-testid': 'bath'})\n",
    "        chair = listing.find('span', {'data-testid': 'chair'})\n",
    "        if bed:\n",
    "            print(bed.find_next('p').getText())\n",
    "        else:\n",
    "            print('1')\n",
    "        if bath:\n",
    "            print(bath.find_next('p').getText())\n",
    "        else:\n",
    "            print('1')\n",
    "        if chair:\n",
    "            print(chair.find_next('p').getText())\n",
    "        else:\n",
    "            print('0')\n",
    "\n",
    "        #Links\n",
    "        link = listing.find('a', {'data-testid': 'listing-details-link'}).attrs['href']\n",
    "        print(link)\n",
    "\n",
    "        #Title\n",
    "        title = listing.find('h2', {'class': 'e2uk8e14'}).getText()\n",
    "        print(title)\n",
    "\n",
    "        #Address\n",
    "        address = listing.find('h2', {'class': 'e2uk8e14'}).find_next('p').getText()\n",
    "        print(address)\n",
    "\n",
    "        #Transportation\n",
    "        transport_container = listing.find('div', {'class': 'css-braguw-TransportWrapper'}).find_all('div', {'class': 'ejjz7ko0'})\n",
    "        transp1, transp2 = transport_container[:2]\n",
    "\n",
    "        transport1 = transp1.attrs['content']\n",
    "        transport2 = transp2.attrs['content']\n",
    "\n",
    "        print(transport1)\n",
    "        print(transport2)\n",
    "\n",
    "        transport_type_1 = transp1.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "        transport_type_2 = transp2.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "\n",
    "        print(transport_type_1)\n",
    "        print(transport_type_2)\n",
    "\n",
    "        #Parking\n",
    "        parking = listing.find('div', {'content': 'Parking'})\n",
    "        if parking:\n",
    "            print(1)\n",
    "        else:\n",
    "            print(0)\n",
    "\n",
    "        #Tags\n",
    "        tag = listings[0].find('span', {'class': 'css-10ddrfy-Tag-StyledTag'})\n",
    "        if tag:\n",
    "            print(tag.getText())\n",
    "        else:\n",
    "            print('None')\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'{str(err)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_test('islington-london-borough', 49)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
