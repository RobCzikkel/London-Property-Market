{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "* Scrape listings\n",
    "* List of Boroughs\n",
    "* Data on crime rates, salaries etc... (Kaggle, Google, Gov stats)\n",
    "* Save it all to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping all boroughs process:\n",
    "* ~~scrape boroughs into a list~~\n",
    "    * loop through boroughs list to create urls\n",
    "        * get number of pages with each url\n",
    "            * loop through listings with range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city-of-london-london-borough',\n",
       " 'city-of-westminster',\n",
       " 'kensington-and-chelsea-royal-borough',\n",
       " 'hammersmith-and-fulham-london-borough',\n",
       " 'wandsworth-london-borough',\n",
       " 'lambeth-london-borough',\n",
       " 'southwark-london-borough',\n",
       " 'tower-hamlets-london-borough',\n",
       " 'hackney-london-borough',\n",
       " 'islington-london-borough',\n",
       " 'camden-london-borough',\n",
       " 'brent-london-borough',\n",
       " 'ealing-london-borough',\n",
       " 'hounslow-london-borough',\n",
       " 'richmond-upon-thames-london-borough',\n",
       " 'kingston-upon-thames',\n",
       " 'merton-london-borough',\n",
       " 'sutton-london-borough',\n",
       " 'croydon-london-borough',\n",
       " 'bromley-london-borough',\n",
       " 'lewisham-london-borough',\n",
       " 'greenwich-royal-borough',\n",
       " 'bexley-london-borough',\n",
       " 'havering-london-borough',\n",
       " 'barking-and-dagenham-london-borough',\n",
       " 'redbridge-london-borough',\n",
       " 'newham-london-borough',\n",
       " 'waltham-forest-london-borough',\n",
       " 'haringey-london-borough',\n",
       " 'enfield-london-borough',\n",
       " 'barnet-london-borough',\n",
       " 'harrow-london-borough',\n",
       " 'hillingdon-london-borough']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_url = 'https://en.wikipedia.org/wiki/London_boroughs'\n",
    "wiki = requests.get(wiki_url)\n",
    "wiki_soup = BeautifulSoup(wiki.content, 'html.parser')\n",
    "ols = wiki_soup.select('td > ol')\n",
    "boroughs = []\n",
    "for ol in ols:\n",
    "    lis = ol.find_all('li')\n",
    "    for li in lis:\n",
    "        boroughs.append(li.find('a').getText())\n",
    "        \n",
    "# List of boroughs\n",
    "boroughs = [x.replace(' ', '-').lower() + '-london-borough' for x in boroughs]\n",
    "# Exceptions treatment\n",
    "boroughs[boroughs.index('kensington-and-chelsea-london-borough')] = 'kensington-and-chelsea-royal-borough'\n",
    "boroughs[boroughs.index('city-of-westminster-london-borough')] = 'city-of-westminster'\n",
    "boroughs[boroughs.index('kingston-upon-thames-london-borough')] = 'kingston-upon-thames'\n",
    "boroughs[boroughs.index('greenwich-london-borough')] = 'greenwich-royal-borough'\n",
    "boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "borough_list = []\n",
    "price_list = []\n",
    "bed_list = []\n",
    "bath_list = []\n",
    "chair_list = []\n",
    "link_list = []\n",
    "title_list = []\n",
    "address_list = []\n",
    "transport1_list = []\n",
    "transport2_list = []\n",
    "transport_type_1_list = []\n",
    "transport_type_2_list = []\n",
    "tag_list = []\n",
    "parking_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the links\n",
    "links = []\n",
    "pages = []\n",
    "for borough in boroughs:\n",
    "    links.append(f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn=1')\n",
    "\n",
    "# Looping through all links and scraoing the listings\n",
    "for link in links:\n",
    "    try:\n",
    "        data = requests.get(link).content\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        last_page_nr = int(soup.find_all('li', {'class': 'eaoxhri1'})[-1].find('a').getText()) + 1\n",
    "        pages.append(last_page_nr)\n",
    "    except:\n",
    "        print(link)\n",
    "        \n",
    "len(links) == len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting city-of-london-london-borough\n",
      "city-of-london-london-borough has been scraped\n",
      "Starting city-of-westminster\n",
      "city-of-westminster has been scraped\n",
      "Starting kensington-and-chelsea-royal-borough\n",
      "kensington-and-chelsea-royal-borough has been scraped\n",
      "Starting hammersmith-and-fulham-london-borough\n",
      "hammersmith-and-fulham-london-borough has been scraped\n",
      "Starting wandsworth-london-borough\n",
      "wandsworth-london-borough has been scraped\n",
      "Starting lambeth-london-borough\n",
      "lambeth-london-borough has been scraped\n",
      "Starting southwark-london-borough\n",
      "southwark-london-borough has been scraped\n",
      "Starting tower-hamlets-london-borough\n",
      "tower-hamlets-london-borough has been scraped\n",
      "Starting hackney-london-borough\n",
      "hackney-london-borough has been scraped\n",
      "Starting islington-london-borough\n",
      "islington-london-borough has been scraped\n",
      "Starting camden-london-borough\n",
      "camden-london-borough has been scraped\n",
      "Starting brent-london-borough\n",
      "brent-london-borough has been scraped\n",
      "Starting ealing-london-borough\n",
      "ealing-london-borough has been scraped\n",
      "Starting hounslow-london-borough\n",
      "hounslow-london-borough has been scraped\n",
      "Starting richmond-upon-thames-london-borough\n",
      "richmond-upon-thames-london-borough has been scraped\n",
      "Starting kingston-upon-thames\n",
      "kingston-upon-thames has been scraped\n",
      "Starting merton-london-borough\n",
      "merton-london-borough, page 9, listing listing_58466565 has an error 'NoneType' object has no attribute 'find_all'\n",
      "merton-london-borough has been scraped\n",
      "Starting sutton-london-borough\n",
      "sutton-london-borough has been scraped\n",
      "Starting croydon-london-borough\n",
      "croydon-london-borough, page 12, listing listing_58537668 has an error 'NoneType' object has no attribute 'find_all'\n",
      "croydon-london-borough has been scraped\n",
      "Starting bromley-london-borough\n",
      "bromley-london-borough has been scraped\n",
      "Starting lewisham-london-borough\n",
      "lewisham-london-borough has been scraped\n",
      "Starting greenwich-royal-borough\n",
      "greenwich-royal-borough has been scraped\n",
      "Starting bexley-london-borough\n",
      "bexley-london-borough has been scraped\n",
      "Starting havering-london-borough\n",
      "havering-london-borough has been scraped\n",
      "Starting barking-and-dagenham-london-borough\n",
      "barking-and-dagenham-london-borough has been scraped\n",
      "Starting redbridge-london-borough\n",
      "redbridge-london-borough has been scraped\n",
      "Starting newham-london-borough\n",
      "newham-london-borough has been scraped\n",
      "Starting waltham-forest-london-borough\n",
      "waltham-forest-london-borough has been scraped\n",
      "Starting haringey-london-borough\n",
      "haringey-london-borough has been scraped\n",
      "Starting enfield-london-borough\n",
      "enfield-london-borough has been scraped\n",
      "Starting barnet-london-borough\n",
      "barnet-london-borough has been scraped\n",
      "Starting harrow-london-borough\n",
      "harrow-london-borough has been scraped\n",
      "Starting hillingdon-london-borough\n",
      "hillingdon-london-borough has been scraped\n"
     ]
    }
   ],
   "source": [
    "for i, borough in enumerate(boroughs):\n",
    "    print(f'Starting {borough}')\n",
    "    for n in range(1, pages[i]):        \n",
    "        try:\n",
    "            base_url = f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn={n}'\n",
    "            data = requests.get(base_url).content\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            listings_container = soup.find('div', {'class': 'css-kdnpqc-ListingsContainer'})\n",
    "            listings = listings_container.find_all('div', {'class': 'e2uk8e18'})\n",
    "            regex = re.compile(r'search-result_listing_\\d+')\n",
    "            listing_ids = soup.find_all('div', {'data-testid': regex})\n",
    "            \n",
    "            #For loop for the listings list\n",
    "            for i, listing in enumerate(listings):\n",
    "                try:\n",
    "                    id_num = listing_ids[i].attrs['id']\n",
    "                    ids.append(id_num)\n",
    "                    #Borough\n",
    "                    borough_list.append(borough)\n",
    "                    #Price\n",
    "                    price = listing.find('p', {'class': 'eczcs4p0'}).getText()\n",
    "                    price_list.append(price)\n",
    "\n",
    "                    #Features\n",
    "                    bed = listing.find('span', {'data-testid': 'bed'})\n",
    "                    bath = listing.find('span', {'data-testid': 'bath'})\n",
    "                    chair = listing.find('span', {'data-testid': 'chair'})\n",
    "                    if bed:\n",
    "                        bed_list.append(bed.find_next('p').getText())\n",
    "                    else:\n",
    "                        bed_list.append('1')\n",
    "                    if bath:\n",
    "                        bath_list.append(bath.find_next('p').getText())\n",
    "                    else:\n",
    "                        bath_list.append('1')\n",
    "                    if chair:\n",
    "                        chair_list.append(chair.find_next('p').getText())\n",
    "                    else:\n",
    "                        chair_list.append('0')\n",
    "\n",
    "                    #Links\n",
    "                    link = listing.find('a', {'data-testid': 'listing-details-link'}).attrs['href']\n",
    "                    link_list.append(link)\n",
    "\n",
    "                    #Title\n",
    "                    title = listing.find('h2', {'class': 'e2uk8e14'}).getText()\n",
    "                    title_list.append(title)\n",
    "\n",
    "                    #Address\n",
    "                    address = listing.find('h2', {'class': 'e2uk8e14'}).find_next('p').getText()\n",
    "                    address_list.append(address)\n",
    "\n",
    "                    #Transportation\n",
    "                    transport_container = listing.find('div', {'class': 'css-braguw-TransportWrapper'}).find_all('div', {'class': 'ejjz7ko0'})\n",
    "                    transp1, transp2 = transport_container[:2]\n",
    "\n",
    "                    transport1 = transp1.attrs['content']\n",
    "                    transport2 = transp2.attrs['content']\n",
    "\n",
    "                    transport1_list.append(transport1)\n",
    "                    transport2_list.append(transport2)\n",
    "\n",
    "                    transport_type_1 = transp1.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "                    transport_type_2 = transp2.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "\n",
    "                    transport_type_1_list.append(transport_type_1)\n",
    "                    transport_type_2_list.append(transport_type_2)\n",
    "\n",
    "                    #Parking\n",
    "                    parking = listing.find('div', {'content': 'Parking'})\n",
    "                    if parking:\n",
    "                        parking_list.append(1)\n",
    "                    else:\n",
    "                        parking_list.append(0)\n",
    "\n",
    "                    #Tags\n",
    "                    tag = listings[0].find('span', {'class': 'css-10ddrfy-Tag-StyledTag'})\n",
    "                    if tag:\n",
    "                        tag_list.append(tag.getText())\n",
    "                    else:\n",
    "                        tag_list.append('None')\n",
    "                        \n",
    "                except:\n",
    "                    print(f'Problem at {id_num} on page {n} of {borough}')\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        except Exception as err:\n",
    "            print(f'{borough}, page {n}, listing {id_num} has an error {str(err)}')\n",
    "            \n",
    "\n",
    "    print(f'{borough} has been scraped')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_failed(borough, n, id_num):\n",
    "    base_url = f'https://www.zoopla.co.uk/for-sale/property/{borough}/?page_size=25&q={borough}&radius=0&results_sort=newest_listings&pn={n}'\n",
    "    print(base_url)\n",
    "    data = requests.get(base_url).content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    listing = soup.select_one('div', {'id': id_num})\n",
    "\n",
    "    #For loop for the listings list\n",
    "\n",
    "    try:\n",
    "        print(id_num)\n",
    "        ids.append(id_num)\n",
    "        #Borough\n",
    "        print(borough)\n",
    "        borough_list.append(borough)\n",
    "        #Price\n",
    "        price = listing.find('p', {'class': 'eczcs4p0'}).getText()\n",
    "        print(price)\n",
    "        price_list.append(price)\n",
    "\n",
    "        #Features\n",
    "        bed = listing.find('span', {'data-testid': 'bed'})\n",
    "        bath = listing.find('span', {'data-testid': 'bath'})\n",
    "        chair = listing.find('span', {'data-testid': 'chair'})\n",
    "        if bed:\n",
    "            print(bed.find_next('p').getText())\n",
    "            bed_list.append(bed.find_next('p').getText())\n",
    "        else:\n",
    "            print('1')\n",
    "            bed_list.append('1')\n",
    "        if bath:\n",
    "            print(bath.find_next('p').getText())\n",
    "            bath_list.append(bath.find_next('p').getText())\n",
    "        else:\n",
    "            print('1')\n",
    "            bath_list.append('1')\n",
    "        if chair:\n",
    "            print(chair.find_next('p').getText())\n",
    "            chair_list.append(chair.find_next('p').getText())\n",
    "        else:\n",
    "            print('0')\n",
    "            chair_list.append('0')\n",
    "\n",
    "        #Links\n",
    "        link = listing.find('a', {'data-testid': 'listing-details-link'}).attrs['href']\n",
    "        print(link)\n",
    "        link_list.append(link)\n",
    "\n",
    "        #Title\n",
    "        title = listing.find('h2', {'class': 'e2uk8e14'}).getText()\n",
    "        print(title)\n",
    "        title_list.append(title)\n",
    "\n",
    "        #Address\n",
    "        address = listing.find('h2', {'class': 'e2uk8e14'}).find_next('p').getText()\n",
    "        print(address)\n",
    "        address_list.append(address)\n",
    "\n",
    "        #Transportation\n",
    "        transport_container = listing.find('div', {'class': 'css-braguw-TransportWrapper'}).find_all('div', {'class': 'ejjz7ko0'})\n",
    "        transp1, transp2 = transport_container[:2]\n",
    "\n",
    "        transport1 = transp1.attrs['content']\n",
    "        transport2 = transp2.attrs['content']\n",
    "\n",
    "        print(transport1)\n",
    "        print(transport2)\n",
    "        transport1_list.append(transport1)\n",
    "        transport2_list.append(transport2)\n",
    "\n",
    "        transport_type_1 = transp1.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "        transport_type_2 = transp2.find('span', {'role': 'presentation'}).attrs['data-testid']\n",
    "\n",
    "        print(transport_type_1)\n",
    "        print(transport_type_2)\n",
    "        transport_type_1_list.append(transport_type_1)\n",
    "        transport_type_2_list.append(transport_type_2)\n",
    "\n",
    "        #Parking\n",
    "        parking = listing.find('div', {'content': 'Parking'})\n",
    "        if parking:\n",
    "            print(1)\n",
    "            parking_list.append(1)\n",
    "        else:\n",
    "            print(0)\n",
    "            parking_list.append(0)\n",
    "\n",
    "        #Tags\n",
    "        tag = listings[0].find('span', {'class': 'css-10ddrfy-Tag-StyledTag'})\n",
    "        if tag:\n",
    "            print(tag.getText())\n",
    "            tag_list.append(tag.getText())\n",
    "        else:\n",
    "            print('None')\n",
    "            tag_list.append('None')\n",
    "\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'{str(err)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.zoopla.co.uk/for-sale/property/merton-london-borough/?page_size=25&q=merton-london-borough&radius=0&results_sort=newest_listings&pn=9\n",
      "listing_58466565\n",
      "merton-london-borough\n",
      "merton-london-boroughThis area onlyAny bedsAny priceShow all\n",
      "1\n",
      "1\n",
      "1\n",
      "/for-sale/details/58466501/?search_identifier=901380155c60670666d31c5f79ebee94\n",
      "1 bed flat for sale\n",
      "Hartfield Road, London SW19\n",
      "0.2 miles Wimbledon\n",
      "0.2 miles Wimbledon\n",
      "london_underground_station\n",
      "national_rail_station\n",
      "0\n",
      "Investment\n"
     ]
    }
   ],
   "source": [
    "add_failed('merton-london-borough', 9, 'listing_58466565')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.zoopla.co.uk/for-sale/property/croydon-london-borough/?page_size=25&q=croydon-london-borough&radius=0&results_sort=newest_listings&pn=12\n",
      "listing_58537668\n",
      "croydon-london-borough\n",
      "croydon-london-boroughThis area onlyAny bedsAny priceShow all\n",
      "2\n",
      "1\n",
      "1\n",
      "/for-sale/details/58537886/?search_identifier=a696daa2e5328eafed78684744c4d5f3\n",
      "2 bed flat for sale\n",
      "Normanton Road, South Croydon CR2\n",
      "0.3 miles South Croydon\n",
      "0.8 miles Sanderstead\n",
      "national_rail_station\n",
      "national_rail_station\n",
      "1\n",
      "Investment\n"
     ]
    }
   ],
   "source": [
    "add_failed('croydon-london-borough', 12, 'listing_58537668')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71154\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'borough': borough_list,\n",
    "    'id': ids,\n",
    "    'address': address_list,\n",
    "    'link': link_list,\n",
    "    'title': title_list,\n",
    "    'num_of_bed': bed_list,\n",
    "    'num_of_bath': bath_list,\n",
    "    'reception_room': chair_list,\n",
    "    'transport_primary': transport_type_1_list,\n",
    "    'station1_dist(mi)': transport1_list,\n",
    "    'transport_secondary': transport_type_2_list,\n",
    "    'station2_dist(mi)': transport2_list,\n",
    "    'tag': tag_list,\n",
    "    'parking': parking_list,\n",
    "    'price': price_list,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ldn_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
